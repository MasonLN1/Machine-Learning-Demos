{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "IVhm9pfqKmGp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x1, x2, mu=1):\n",
        "   return x1-2*x2-mu*math.log(1+x1-x2**2)-mu*math.log(x2) # Beta_u\n",
        "def g(x1, x2, mu=1):\n",
        "   return np.array([1-mu/(1+x1-x2**2), -2+(2*mu*x2)/(1+x1-x2**2)-mu/x2]) # Gradient of Beta_u\n",
        "\n",
        "initial = (2,1)"
      ],
      "metadata": {
        "id": "oCS47rC7KWhK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_method_fixed(f, g, x0, t, epsilon):\n",
        "  # f = input function\n",
        "  # g = gradient of f\n",
        "  # x0 = initial point\n",
        "  # t = fixed step size\n",
        "  # epsilon = tolerance\n",
        "\n",
        "  x=np.array(x0)\n",
        "  if x.ndim == 0:\n",
        "        x = np.array([x]) # To ensure no TypeError if f is a single variable function (not necessary for this problem)\n",
        "  grad=g(*x)\n",
        "  iter=0\n",
        "  while (np.linalg.norm(grad) > epsilon):\n",
        "    iter=iter+1\n",
        "    x=x-t*grad\n",
        "    fun_val=f(*x)\n",
        "    grad=g(*x)\n",
        "    # print(f\"Iteration #{iter}: norm_grad = {np.linalg.norm(grad):.5f}, fun_val = {fun_val:.7f}\")\n",
        "  return x"
      ],
      "metadata": {
        "id": "BrY1SXi5MX0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrapper(x0=initial, t=0.005, epsilon=0.00001):\n",
        "  x_values = []\n",
        "  for mu in (1, 0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625): # The set of mu can be changed to whatever we want\n",
        "\n",
        "      f_mu = lambda x, y: f(x, y, mu) # Define f and g as Beta_u and Grad(Beta_u) with the mu value defined based on where we are in the loop.\n",
        "      g_mu = lambda x, y: g(x, y, mu)\n",
        "\n",
        "      x_values.append(gradient_method_fixed(f_mu, g_mu, x0, t, epsilon)) # Save our iteration's value that we converged to. This is ultimately unnecessary unless we want the function to return our x_values.\n",
        "      x0 = gradient_method_fixed(f_mu, g_mu, x0, t, epsilon) # Use the value we converged to as our next initial guess before the loop restarts.\n",
        "      print(f\"For mu={mu}: x* ≈ {x0}\")\n",
        "  return #x_values"
      ],
      "metadata": {
        "id": "hwjDl2iGNDFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfdsiXUMOr5z",
        "outputId": "6754c0f3-de62-44d9-b0b9-a6fa19053cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For mu=1: x* ≈ [1.86606628 1.36603684]\n",
            "For mu=0.5: x* ≈ [0.95713837 1.20711791]\n",
            "For mu=0.25: x* ≈ [0.48739935 1.1123835 ]\n",
            "For mu=0.125: x* ≈ [0.24654158 1.05902807]\n",
            "For mu=0.0625: x* ≈ [0.12410354 1.03034119]\n",
            "For mu=0.03125: x* ≈ [0.06228612 1.01539935]\n",
            "For mu=0.015625: x* ≈ [0.0312125  1.00776354]\n"
          ]
        }
      ]
    }
  ]
}